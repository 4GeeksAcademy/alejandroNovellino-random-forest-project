{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Get the data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the data processed in the previous notebook (Exploratory Data Analysis)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.constants import X_TRAIN_PATH, X_TEST_PATH, Y_TRAIN_PATH, Y_TEST_PATH\n",
    "\n",
    "# save the processed data to their corresponding files\n",
    "X_train = pd.read_csv(filepath_or_buffer=X_TRAIN_PATH, sep=',')\n",
    "X_test = pd.read_csv(filepath_or_buffer=X_TEST_PATH, sep=',')\n",
    "\n",
    "y_train = pd.read_csv(filepath_or_buffer=Y_TRAIN_PATH, sep=',')\n",
    "y_test = pd.read_csv(filepath_or_buffer=Y_TEST_PATH, sep=',')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify x_train\n",
    "X_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify x_test\n",
    "X_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify y_train\n",
    "y_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify y_test\n",
    "y_test.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Option 1. Default model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialization and training of the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#from sklearn import ModelType\n",
    "\n",
    "model = ModelType(random_state = 42)\n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Hyperparameters of the default model: {model.get_params()}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction of the probabilities of being one class or another\n",
    "y_prob = model.predict_proba(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's print the full report of the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            log_loss, classification_report)\n",
    "\n",
    "default_model_accuracy = accuracy_score(y_test, y_pred)\n",
    "default_model_precision = precision_score(y_test, y_pred)\n",
    "default_model_recall = recall_score(y_test, y_pred)\n",
    "default_model_f1 = f1_score(y_test, y_pred)\n",
    "default_model_auc_roc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "default_model_log_loss = log_loss(y_test, y_prob)\n",
    "default_model_confusion = confusion_matrix(y_test, y_pred)\n",
    "default_model_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {default_model_accuracy}')\n",
    "print(f'Precision: {default_model_precision}')\n",
    "print(f'Recall: {default_model_recall}')\n",
    "print(f'F1-Score: {default_model_f1}')\n",
    "print(f'AUC-ROC: {default_model_auc_roc}')\n",
    "print(f'Confusion Matrix:\\n{default_model_confusion}')\n",
    "print(f'Log Loss: {default_model_log_loss}')\n",
    "print(f'Classification Report:\\n{default_model_report}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here the analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let's draw the confusion matrix"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.draw_utils import draw_confusion_matrix\n",
    "\n",
    "draw_confusion_matrix(confusion=default_model_confusion)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of a confusion matrix is as follows:\n",
    "\n",
    "- **True positive (TP)**: corresponds to the number 40 and are the cases where the model predicted positive **(the person has diabetes)** and the actual class is also positive.\n",
    "- **True negative (TN)**: Corresponds to the number 75 and are the cases where the model predicted negative **(the person does not have diabetes)** and the actual class is also negative.\n",
    "- **False positive (FP)**: Corresponds to the number 24 and are the cases in which the model predicted positive, but the actual class is negative.\n",
    "- **False negative (FN)**: Corresponds to the number 15 and are the cases where the model predicted negative, but the actual class is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2. Model with optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# create another model\n",
    "opt_model = ModelType(random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Create the hyperparameter optimization model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the parameters that we want to adjust by hand, depends on the model to use\n",
    "hyperparams = {\n",
    "\n",
    "}\n",
    "\n",
    "# initialize the grid\n",
    "grid = GridSearchCV(opt_model, hyperparams, scoring = 'accuracy', cv = 5, n_jobs=-1, verbose=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "grid.fit(X_train, y_train.values)\n",
    "\n",
    "print(f\"Best hyperparameters: {grid.best_params_}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get the best model and predict"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get the best parameters and model\n",
    "best_params = grid.best_params_\n",
    "best_model: ModelType = grid.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# predict the values\n",
    "y_pred = best_model.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# make the prediction of the probabilities of being one class or another\n",
    "y_prob = best_model.predict_proba(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Metrics of the model"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            log_loss, classification_report)\n",
    "\n",
    "optimized_model_accuracy = accuracy_score(y_test, y_pred)\n",
    "optimized_model_precision = precision_score(y_test, y_pred)\n",
    "optimized_model_recall = recall_score(y_test, y_pred)\n",
    "optimized_model_f1 = f1_score(y_test, y_pred)\n",
    "optimized_model_auc_roc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "optimized_model_log_loss = log_loss(y_test, y_prob)\n",
    "optimized_model_confusion = confusion_matrix(y_test, y_pred)\n",
    "optimized_model_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {optimized_model_accuracy}')\n",
    "print(f'Precision: {optimized_model_precision}')\n",
    "print(f'Recall: {optimized_model_recall}')\n",
    "print(f'F1-Score: {optimized_model_f1}')\n",
    "print(f'AUC-ROC: {optimized_model_auc_roc}')\n",
    "print(f'Confusion Matrix:\\n{optimized_model_confusion}')\n",
    "print(f'Log Loss: {optimized_model_log_loss}')\n",
    "print(f'Classification Report:\\n{optimized_model_report}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "draw_confusion_matrix(confusion=optimized_model_confusion)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of a confusion matrix is as follows:\n",
    "\n",
    "- **True positive (TP)**: corresponds to the number 42 and are the cases where the model predicted positive **(the person has diabetes)** and the actual class is also positive.\n",
    "- **True negative (TN)**: Corresponds to the number 72 and are the cases where the model predicted negative **(the person does not have diabetes)** and the actual class is also negative.\n",
    "- **False positive (FP)**: Corresponds to the number 27 and are the cases in which the model predicted positive, but the actual class is negative.\n",
    "- **False negative (FN)**: Corresponds to the number 13 and are the cases where the model predicted negative, but the actual class is positive."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from src.markdown_utils import show_comparison_table\n",
    "\n",
    "# set the metrics to use\n",
    "metrics: list[str] = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "\n",
    "# get the metrics of the default and optimized list values\n",
    "default_model_metrics: list[float] = [default_model_accuracy, default_model_precision, default_model_recall, default_model_f1, default_model_auc_roc]\n",
    "optimized_model_metrics: list[float] = [optimized_model_accuracy, optimized_model_precision, optimized_model_recall, optimized_model_f1, optimized_model_auc_roc]\n",
    "\n",
    "# construct the Markdown table\n",
    "show_comparison_table(\n",
    "    metric_names=metrics,\n",
    "    default_metrics=default_model_metrics,\n",
    "    optimized_metrics=optimized_model_metrics,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.draw_utils import draw_comparison_confusion_matrices\n",
    "\n",
    "draw_comparison_confusion_matrices(confusion_1=default_model_confusion, confusion_2=optimized_model_confusion, confusion_matrix_1_name='Default model', confusion_matrix_2_name='Optimized model')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "WE can see that the default model and the optimized one are really close, the metrics are not too different, one model has a better performance with the negative label 0 (do not have diabetes) and other model have better results with the positive label 1 (have diabetes).\n",
    "\n",
    "The default model has a better performance with the label 1, it has better accuracy and precision. While the optimized model has a better performance with the label 0 and these results are backed up by the metrics, because the optimized model has better recall, f1-score and AUC-ROC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
