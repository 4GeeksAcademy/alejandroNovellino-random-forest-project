{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import the libraries top use\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "sns.set_theme()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Problem statement and data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data information in the page <https://raw.githubusercontent.com/4GeeksAcademy/linear-regression-project-tutorial/main/medical_insurance_cost.csv> where each feature is:\n",
    "\n",
    "1. age. Age of primary beneficiary **(numeric)**\n",
    "2. sex. Gender of the primary beneficiary **(categorical)**\n",
    "3. bmi. Body mass index **(numeric)**\n",
    "4. children. Number of children/dependents covered by health insurance **(numeric)**\n",
    "5. smoker. Is the person a smoker? **(categorical)**\n",
    "6. region. Beneficiary's residential area in the U.S.: northeast, southeast, southwest, northwest **(categorical)**\n",
    "7. charges. Health insurance premium **(numerical) (TARGET)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.utils import load_data, ReadCsvParams, SaveCsvParams\n",
    "\n",
    "file_path = '../data/raw/file_name.csv'\n",
    "url = 'url_to_data.csv'\n",
    "read_csv_params: ReadCsvParams = {'delimiter': ','}\n",
    "save_csv_params: SaveCsvParams = {'sep': ','}\n",
    "\n",
    "df: pd.DataFrame = load_data(file_path=file_path, url=url, read_csv_params=read_csv_params, save_csv_params=save_csv_params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem to solve:\n",
    "Calculate, based on the physiological data of its customers, what will be the premium (cost) to be borne by someone. Construct a Linear Regression model to predict the cost of a person to be ensured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploration and data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's see how is the data, the info and a little of its distribution."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# head of the dataframe\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tail of the dataframe\n",
    "df.tail()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# info of the dataframe\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# describe the dataframe\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cols for the different types of data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# numerical columns\n",
    "numerical_cols: list[str] = []\n",
    "\n",
    "# categorical columns\n",
    "categorical_cols: list[str] = []\n",
    "\n",
    "# features\n",
    "features = []\n",
    "\n",
    "# target variable\n",
    "target: str = ''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate duplicates\n",
    "\n",
    "This could be done here, or in the feature engineering step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate irrelevant information\n",
    "\n",
    "This could be done here, or in the feature engineering step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 3: Analysis of uni variate variables"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "A **uni variate variable** is a statistical term used to refer to a set of observations of an attribute. That is, the column-by-column analysis of the DataFrame. To do this, we must distinguish whether a variable is categorical or numerical, as the body of the analysis and the conclusions that can be drawn will be different."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **categorical variable** is a type of variable that can be one of a limited number of categories or groups. These groups are often nominal (e.g., the color of a car: red, blue, black, etc., but none of these colors is inherently \"greater\" or \"better\" than the others) but can also be represented by finite numbers.\n",
    "\n",
    "**To represent these types of variables we will use histograms.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# let's remember the categorical data\n",
    "print(f'Categorical columns: {categorical_cols}')\n",
    "print(f'Amount of categorical columns: {len(categorical_cols)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axis = plt.subplots(1, 3, figsize = (14, 4))\n",
    "\n",
    "\"\"\"\n",
    "Create histograms for each categorical feature to see the count for each categorical feature.\n",
    "\"\"\"\n",
    "\n",
    "# creating a multiple figure with histograms and box plots\n",
    "# first row\n",
    "# \t col\n",
    "sns.histplot(ax = axis[0], data = df, x = \"sex\")\n",
    "# \tsecond col\n",
    "sns.histplot(ax = axis[1], data = df, x = \"smoker\").set(ylabel = None)\n",
    "# \tthird col\n",
    "sns.histplot(ax = axis[2], data = df, x = \"region\").set(ylabel = None)\n",
    "\n",
    "\n",
    "# adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Put here the analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **numeric variable** is a type of variable that can take numeric values (integers, fractions, decimals, negatives, etc.) in an infinite range. A numerical categorical variable can also be a numerical variable. \n",
    "\n",
    "**They are usually represented using a histogram and a boxplot, displayed together.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# let's remember the categorical data\n",
    "print(f'Categorical columns: {numerical_cols}')\n",
    "print(f'Amount of categorical columns: {len(numerical_cols)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axis = plt.subplots(2, 3, figsize = (20, 5), gridspec_kw={'height_ratios': [6, 1]})\n",
    "\n",
    "\"\"\"\n",
    "Create histograms and box-plots for each numerical feature to see the count for each categorical feature.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# creating a multiple figure with histograms and box plots\n",
    "# first row\n",
    "# \t col\n",
    "sns.histplot(ax = axis[0, 0], data = df, x = \"age\").set(xlabel = None)\n",
    "sns.boxplot(ax = axis[1, 0], data = df, x = \"age\")\n",
    "# \tsecond col\n",
    "sns.histplot(ax = axis[0, 1], data = df, x = \"bmi\").set(xlabel = None, ylabel = None)\n",
    "sns.boxplot(ax = axis[1, 1], data = df, x = \"bmi\")\n",
    "# \tthird col\n",
    "sns.histplot(ax = axis[0, 2], data = df, x = \"children\").set(xlabel = None, ylabel = None)\n",
    "sns.boxplot(ax = axis[1, 2], data = df, x = \"children\")\n",
    "\n",
    "\n",
    "# adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the breakdown of the distribution and skewness for each variable:\n",
    "\n",
    "**1. For each variable**\n",
    "\n",
    "* **Histogram:** Example - The distribution appears to be relatively uniform or slightly multimodal. There are no strong peaks or a clear bell shape.\n",
    "* **Box Plot:** Example - The box plot shows a relatively symmetrical distribution with no apparent outliers.\n",
    "* **Distribution:** Example - Not normally distributed. It looks more like a uniform distribution or possibly a multimodal distribution.\n",
    "* **Skewness:** Example - Slightly right-skewed, but not strongly so.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Analysis of multivariate variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the characteristics one by one, it is time to analyze them in relation to the predictor and to themselves, in order to draw clearer conclusions about their relationships and to be able to make decisions about their processing.\n",
    "\n",
    "Thus, if we would like to eliminate a variable due to a high amount of null values or certain outliers, it is necessary to first apply this process to ensure that the elimination of certain values are not critical for the survival of a passenger. For example, the variable Cabin has many null values, and we would have to ensure that there is no relationship between it and survival before eliminating it, since it could be very significant and important for the model and its presence could bias the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical-numerical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the two variables being compared have numerical data, the analysis is said to be numerical-numerical. \n",
    "\n",
    "**Scatter-plots and correlation analysis are used to compare two numerical columns.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# graphs of numerical vs numerical with histograms and scatter plots\n",
    "g = sns.PairGrid(df[numerical_cols])\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.draw_utils import draw_corr_matrix\n",
    "\n",
    "# compute the correlation matrix of the numerical columns\n",
    "corr = df[numerical_cols].corr()\n",
    "\n",
    "# draw the correlation matrix\n",
    "draw_corr_matrix(corr=corr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Here analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical-categorical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the two variables being compared have categorical data, the analysis is said to be categorical-categorical. \n",
    "\n",
    "**Histograms and combinations are used to compare two categorical columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorize the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get a copy of the dataframe with factorize categorical data\n",
    "fact_df = df.copy()\n",
    "\n",
    "# factorize every categorical column unless the target\n",
    "for col in categorical_cols:\n",
    "\tfact_df[col] = pd.factorize(fact_df[col])[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# see the factorized dataframe\n",
    "fact_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compute the correlation matrix of the numerical columns\n",
    "corr = fact_df[categorical_cols].corr()\n",
    "\n",
    "# draw the correlation matrix\n",
    "draw_corr_matrix(corr=corr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations of class with various predictors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# let's remember the numerical data\n",
    "print(f'Numerical columns: {numerical_cols}')\n",
    "print(f'Amount of numerical columns: {len(numerical_cols)}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# let's remember the categorical data\n",
    "print(f'Categorical columns: {categorical_cols}')\n",
    "print(f'Amount of categorical columns: {len(categorical_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axis = plt.subplots(2, 2, figsize = (15, 8))\n",
    "\n",
    "\"\"\"\n",
    "Create histograms for each numerical feature with categorical columns as hues.\n",
    "\"\"\"\n",
    "\n",
    "# first row\n",
    "# \t col\n",
    "sns.histplot(ax = axis[0, 0], data = df, x = \"smoker\", hue=\"sex\")\n",
    "# \tsecond col\n",
    "sns.histplot(ax = axis[0, 1], data = df, x = \"region\", hue=\"sex\").set(ylabel = None)\n",
    "\n",
    "# second row\n",
    "# \tfirst col\n",
    "sns.histplot(ax = axis[1, 0], data = df, x = \"sex\", hue=\"smoker\")\n",
    "# \tsecond col\n",
    "sns.histplot(ax = axis[1, 1], data = df, x = \"region\", hue=\"smoker\").set(ylabel = None)\n",
    "\n",
    "\n",
    "# adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here the analysis."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical-categorical analysis (complete)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now do the analysis of the numerical vs categorical variables (factorized)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# pair-plot of all the data\n",
    "sns.pairplot(data = fact_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compute the correlation matrix of all the data\n",
    "corr = fact_df.corr()\n",
    "\n",
    "# draw the correlation matrix\n",
    "draw_corr_matrix(corr=corr)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the analysis of the relation of all the variables\n",
    "\n",
    "***For example*** We can see that the variables that have the stronger correlation with the target **y** are:\n",
    "\n",
    "Positive correlation:\n",
    "1. variable 1\n",
    "2. variable 2\n",
    "\n",
    "Negative correlation:\n",
    "1. variable 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Feature engineering*** is a process that involves the creation of new features (or variables) from existing ones to improve model performance. This may involve a variety of techniques, such as normalization, data transformation, and so on. The goal is to improve the accuracy of the model and/or reduce the complexity of the model, thus making it easier to interpret.\n",
    "\n",
    "Although this could have been done in this step as it is part of the feature engineering, it is usually done before analyzing the variables, separating this process into a previous one and the one we are going to see next."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missing value analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **missing** value is a space that has no value assigned to it in the observation of a specific variable. These types of values are quite common and can arise for many reasons. For example, there could be an error in data collection, someone may have refused to answer a question in a survey, or it could simply be that certain information is not available or not applicable."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Treating Missing Values in Pandas DataFrames"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Treating missing values in a variable within a Pandas DataFrame is a crucial step in data preprocessing. Here's a breakdown of common methods and when to use them:\n",
    "\n",
    "**1. Removing Rows or Columns:**\n",
    "\n",
    "* **Method:** Delete rows or columns containing missing values.\n",
    "* **When to use:**\n",
    "    * When missing values are a small percentage of the dataset.\n",
    "    * When you're confident that removing the missing values won't introduce significant bias.\n",
    "    * When a column has a very large number of missing values.\n",
    "* **Caution:**\n",
    "    * Can lead to significant data loss, especially if missing values are widespread.\n",
    "    * May introduce bias if missing values are not randomly distributed.\n",
    "\n",
    "\n",
    "**2. Imputation:**\n",
    "\n",
    "* **Method:** Replace missing values with estimated values.\n",
    "* **Common Imputation Methods:**\n",
    "    * **Mean Imputation:** Replace missing values with the mean of the column.\n",
    "    * **Median Imputation:** Replace missing values with the median of the column. (Robust to outliers)\n",
    "    * **Mode Imputation:** Replace missing values with the mode (most frequent value) of the column. (For categorical data)\n",
    "    * **Forward Fill:** Propagate the last valid observation forward.\n",
    "    * **Backward Fill:** Propagate the next valid observation backward.\n",
    "    * **Interpolation:** Estimate missing values based on surrounding values.\n",
    "    * **Predictive Imputation:** Use machine learning models to predict missing values.\n",
    "* **When to use:**\n",
    "    * When you want to preserve the data and avoid data loss.\n",
    "    * When missing values are likely due to random factors.\n",
    "    * When you have time series data and the forward/backward fill make sense.\n",
    "* **Caution:**\n",
    "    * Can introduce bias if imputed values are not accurate.\n",
    "    * Reduces variability in the data.\n",
    "    * Using the mean can be heavily influenced by outliers.\n",
    "\n",
    "\n",
    "**3. Creating a Missing Value Indicator:**\n",
    "\n",
    "* **Method:** Create a new binary column indicating whether a value is missing or not.\n",
    "* **When to use:**\n",
    "    * When the fact that a value is missing is itself informative.\n",
    "    * When you want to preserve the missing information for your model.\n",
    "* **Caution:**\n",
    "    * Adds a new feature to your dataset.\n",
    "    * May not be necessary if missing values are completely random.\n",
    "\n",
    "\n",
    "**Choosing the Right Method:**\n",
    "\n",
    "* Analyze the nature of missing values: Are they random or systematic?\n",
    "* Consider the percentage of missing values: If it's high, imputation might be necessary.\n",
    "* Think about the impact on your model: Some models can handle missing values better than others.\n",
    "* Use domain knowledge: Your understanding of the data can guide your decision.\n",
    "* Experiment: Try different methods and evaluate their impact on your analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# verify non values\n",
    "fact_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explain what was done**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier analysis"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "An outlier is a data point that deviates significantly from the others. It is a value that is noticeably different from what would be expected given the general trend of the data. These outliers may be caused by errors in data collection, natural variations in the data, or they may be indicative of something significant, such as an anomaly or extraordinary event.\n",
    "\n",
    "Descriptive analysis is a powerful tool for characterizing the data set: the mean, variance and quartiles provide powerful information about each variable. The describe() function of a DataFrame helps us to calculate in a very short time all these values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# verify the distribution again, we are not going to work in the outliers this time\n",
    "fact_df.describe()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Treating Outliers in Pandas DataFrames"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are several ways to treat outliers in a variable within a Pandas DataFrame. The best approach depends on the nature of your data, the extent of the outliers, and your specific analysis goals. Here's a breakdown of common methods:\n",
    "\n",
    "**1. Removing Outliers:**\n",
    "\n",
    "* **Method:** Filter out rows containing outliers based on a defined threshold.\n",
    "* **When to use:**\n",
    "    * When you're confident that the outliers are due to errors or anomalies.\n",
    "    * When you have a large dataset and removing a few outliers won't significantly impact your analysis.\n",
    "    * When you want to prevent outliers from skewing statistical measures.\n",
    "* **Caution:**\n",
    "    * Can lead to data loss.\n",
    "    * May introduce bias if outliers are not random.\n",
    "\n",
    "\n",
    "**2. Capping/Flooring Outliers:**\n",
    "\n",
    "* **Method:** Replace outlier values with a predefined upper or lower limit.\n",
    "* **When to use:**\n",
    "    * When you want to preserve the data but reduce the impact of outliers.\n",
    "    * When outliers are likely due to extreme but valid values.\n",
    "* **Caution:**\n",
    "    * Can distort the distribution of the data.\n",
    "    * Requires careful selection of capping/flooring limits.\n",
    "\n",
    "\n",
    "**3. Transforming Outliers:**\n",
    "\n",
    "* **Method:** Apply mathematical transformations (e.g., log, square root, Box-Cox) to reduce the skewness caused by outliers.\n",
    "* **When to use:**\n",
    "    * When outliers are causing significant skewness in the data.\n",
    "    * When your model assumes a normal distribution.\n",
    "* **Caution:**\n",
    "    * Can make data interpretation more complex.\n",
    "    * Requires careful selection of transformation methods.\n",
    "\n",
    "\n",
    "**4. Imputing Outliers:**\n",
    "\n",
    "* **Method:** Replace outliers with estimated values (e.g., mean, median).\n",
    "* **When to use:**\n",
    "    * When you want to preserve the data and avoid data loss.\n",
    "    * When the outliers are probably errors.\n",
    "* **Caution:**\n",
    "    * Can introduce bias if imputed values are not accurate.\n",
    "    * Reduces variability in the data.\n",
    "\n",
    "**5. Using Robust Scalers:**\n",
    "\n",
    "* **Method:** Use scaling techniques that are less sensitive to outliers (e.g., `RobustScaler` from scikit-learn).\n",
    "* **When to use:**\n",
    "    * When you want to scale the data without removing or capping outliers.\n",
    "    * When using models that are sensitive to feature scaling.\n",
    "* **Caution:**\n",
    "    * Doesn't remove or modify outliers; it only scales them.\n",
    "\n",
    "**Choosing the Right Method:**\n",
    "\n",
    "* Visualize your data: Use box plots, histograms, and scatter plots to identify outliers.\n",
    "* Consider your model: Some models are more sensitive to outliers than others.\n",
    "* Domain knowledge: Use your understanding of the data to determine the best approach.\n",
    "* Experiment: Try different methods and evaluate their impact on your analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Explain what was done**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another typical use of this engineering is to obtain new features by \"merging\" two or more existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Explain what was done**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the set into train and test,"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.utils import split_my_data\n",
    "\n",
    "\n",
    "# set independent and dependent variables\n",
    "X: pd.DataFrame = fact_df.drop(target, axis = 1)\n",
    "y: pd.Series = fact_df[target]\n",
    "\n",
    "# divide the dataset into training and test samples\n",
    "X_train, X_test, y_train, y_test = split_my_data(X, y, test_size = 0.2, random_state = 42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling** is a crucial step in data preprocessing for many Machine Learning algorithms. It is a technique that changes the range of data values so that they can be compared to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling with Scikit-learn (sklearn)\n",
    "\n",
    "Scikit-learn (sklearn) provides several tools for feature scaling, each with its own characteristics and use cases. Here's a breakdown:\n",
    "\n",
    "**1. StandardScaler:**\n",
    "\n",
    "* **How it works:** Standardizes features by removing the mean and scaling to unit variance.\n",
    "* **Formula:** `z = (x - u) / s`, where `u` is the mean and `s` is the standard deviation.\n",
    "* **When to use:**\n",
    "    * When your data has a Gaussian (normal) distribution, or you want to transform it to resemble a Gaussian distribution.\n",
    "    * When your model assumes that features are centered around zero and have unit variance (e.g., linear regression, logistic regression, support vector machines).\n",
    "* **Caution:** Sensitive to outliers.\n",
    "\n",
    "**2. MinMaxScaler:**\n",
    "\n",
    "* **How it works:** Scales features to a given range, usually between 0 and 1.\n",
    "* **Formula:** `x_scaled = (x - x_min) / (x_max - x_min)`\n",
    "* **When to use:**\n",
    "    * When you need to keep the values within a specific range.\n",
    "    * When you don't have many outliers.\n",
    "    * When using algorithms that are sensitive to the magnitude of features (e.g., neural networks).\n",
    "* **Caution:** Sensitive to outliers.\n",
    "\n",
    "**3. RobustScaler:**\n",
    "\n",
    "* **How it works:** Scales features using statistics that are robust to outliers (median and interquartile range).\n",
    "* **Formula:** `x_scaled = (x - median) / IQR`, where `IQR` is the interquartile range.\n",
    "* **When to use:**\n",
    "    * When your data contains outliers.\n",
    "    * When you want to reduce the impact of outliers on your scaling.\n",
    "* **Caution:** Doesn't normalize data to a specific range.\n",
    "\n",
    "**4. MaxAbsScaler:**\n",
    "\n",
    "* **How it works:** Scales features by dividing each value by the maximum absolute value.\n",
    "* **Formula:** `x_scaled = x / abs(x_max)`\n",
    "* **When to use:**\n",
    "    * When you have sparse data (data with many zero values).\n",
    "    * When you want to preserve the sparsity of your data.\n",
    "    * When you want to scale data to the range [-1, 1].\n",
    "* **Caution:** Sensitive to outliers in the maximum absolute values.\n",
    "\n",
    "**5. QuantileTransformer:**\n",
    "\n",
    "* **How it works:** Transforms features to follow a uniform or normal distribution. It is a non-linear transformation.\n",
    "* **When to use:**\n",
    "    * When your data has a non-linear distribution.\n",
    "    * When you want to reduce the impact of outliers.\n",
    "    * Can also compress outliers into a smaller interval.\n",
    "* **Caution:** Distorts correlations and distances.\n",
    "\n",
    "**6. PowerTransformer:**\n",
    "\n",
    "* **How it works:** Applies power transformations (Yeo-Johnson or Box-Cox) to make data more Gaussian-like.\n",
    "* **When to use:**\n",
    "    * When your data is skewed, and you want to normalize its distribution.\n",
    "    * When your model assumes a Gaussian distribution.\n",
    "* **Caution:** Works better for positive data. Box-Cox can only be used with strictly positive data.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "* **Model Requirements:** The choice of scaler often depends on the requirements of your machine learning model. Some models are more sensitive to the scale of features than others.\n",
    "* **Data Distribution:** Consider the distribution of your data (e.g., Gaussian, skewed, presence of outliers) when choosing a scaler.\n",
    "* **Outliers:** If your data contains outliers, `RobustScaler` or `QuantileTransformer` are good choices.\n",
    "* **Range Requirements:** If you need to scale data to a specific range (e.g., [0, 1] or [-1, 1]), use `MinMaxScaler` or `MaxAbsScaler`.\n",
    "* **Pipelines and ColumnTransformer:** It is highly recommended to use the scikit-learn pipeline, and the ColumnTransformer to properly work with data that have different kind of data into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In this case we are going to use a **StandardScaler** because we do not have many outliers in the features, the outliers are in the target."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# numerical columns without the target\n",
    "numerical_features = ['age', 'bmi', 'children']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# scaler instance\n",
    "scaler = StandardScaler ()\n",
    "\n",
    "### scaling training data --------------------------\n",
    "\n",
    "# crate a copy of the train dataframe\n",
    "X_train_scaled: pd.DataFrame = X_train.copy()\n",
    "# scale just the numerical columns\n",
    "X_train_scaled[numerical_features] = scaler.fit_transform(X_train_scaled[numerical_features])\n",
    "\n",
    "### scaling testing data --------------------------\n",
    "\n",
    "# crate a copy of the test dataframe\n",
    "X_test_scaled: pd.DataFrame = X_test.copy()\n",
    "# scale just the numerical columns\n",
    "X_test_scaled[numerical_features] = scaler.fit_transform(X_test_scaled[numerical_features])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print the head of the x train data\n",
    "X_train_scaled.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print the head of the x test data\n",
    "X_test_scaled.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection is a process that involves selecting the most relevant features (variables) from our dataset to use in building a Machine Learning model, discarding the rest.\n",
    "\n",
    "There are several reasons to include it in our exploratory analysis:\n",
    "\n",
    "1. To simplify the model so that it is easier to understand and interpret.\n",
    "2. To reduce the training time of the model.\n",
    "3. Avoid overfitting by reducing the dimensionality of the model and minimizing noise and unnecessary correlations.\n",
    "4. Improve model performance by removing irrelevant features.\n",
    " \n",
    "In addition, there are several techniques for feature selection. Many of them are based on trained supervised or clustering models. More information is available here.\n",
    "\n",
    "The sklearn library contains many of the best alternatives to perform it. One of the most commonly used tools for fast and successful feature selection processes is SelectKBest. This function selects the k best features from our dataset based on a function of a statistical test. This statistical test is usually an ANOVA or a Chi-Square."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# create the selection model, we are not setting the K value because the results of the models are not great.\n",
    "selection_model = SelectKBest()\n",
    "# fit the model to the train scaled data\n",
    "selection_model.fit(X_train_scaled, y_train)\n",
    "# get the indexes of the selected columns\n",
    "ix = selection_model.get_support()\n",
    "\n",
    "# get the dataframe of the train selected columns\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_scaled), columns = X_train_scaled.columns.values[ix]) # type: ignore\n",
    "\n",
    "# get the dataframe of the test selected columns\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_scaled), columns = X_test_scaled.columns.values[ix]) # type: ignore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print the selected features on the training data\n",
    "X_train_sel.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print the selected features on the test data\n",
    "X_test_sel.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we got different features that the ones that have the best correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Save the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from src.constants import X_TRAIN_PATH, X_TEST_PATH, Y_TRAIN_PATH, Y_TEST_PATH\n",
    "\n",
    "# save the processed data to their corresponding files\n",
    "X_train_sel.to_csv(path_or_buf = X_TRAIN_PATH, sep=',', index=False,)\n",
    "X_test_sel.to_csv(path_or_buf = X_TEST_PATH, sep=',', index=False,)\n",
    "\n",
    "y_train.to_csv(path_or_buf = Y_TRAIN_PATH, sep=',', index=False,)\n",
    "y_test.to_csv(path_or_buf = Y_TEST_PATH, sep=',', index=False,)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
